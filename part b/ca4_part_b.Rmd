---
title: "2022 - Data Analytics for Immersive Environments - CA4 - RDBMS & Linear Regression Project"
subtitle: "CA4 Part B - Linear Regression Analysis"
author: "Joe O'Regan"
date: "2023-01-16"
output:
  html_document:
    toc: true
    toc_depth: 3
  pdf_document: default
always_allow_html: true
---

# Statement of Assumptions

Variables to be tested should ideally be numeric for plotting graphs etc.

Average monthly hours gaming (**avg_monthly_hrs_gaming**) would have a positive effect on average monthly expenditure downloadable content (DLC) (**avg_monthly_expenditure_dlc**). The more hours a player plays games, the more inclined they would be to spend money on DLC.

---

# Testing of Assumptions

Assumptions for Linear Regression

1. Independence of observation
2. Normality
3. Linearity
4. Homoscedasticity

**Independence of observation (No autocorrellation)**  

No need to test for hidden relationships between variables when there is only one independent and one dependent variable.

Find the R value or correlation between variables using cor().

age and avg_years_playing_games don't have floating values so are more likely to repeat.

**Normality (Histograms, Shapiro-Wilk Significance Test**

Visually inspect normality with histograms. If the histogram is symmetrical/unimodal, then the data is assumed to be normally distributed.

Shapiro-Wilk Significance test. Visual inspection isn't always reliable. Widely recommended for normality test and more powerful than Kolmogorov-Smirnov (K-S) nomality test.

Need to combine visual inspection and significance test to get good results, as normality test can be sensitive to sample size. Small samples can pass normality tests.

**Linearity**

Any relationship between the independent and dependent variable is linear: the line of best fit through the data points is a straight line and not a curve or grouping factor.

**Homoscedasticity**

Homogeneity of variance. The size of the error in our prediction doesn't change significantly across the values of the independent variable.

---

# Analysis conducted and results obtained

**Correlation**

Correlation between avg_monthly_hrs_gaming and avg_monthly_expenditure_dlc is smallest. There is no apparent linear relationship between the variables.

Correlation between age and avg_yers_playing_games is largest but it is still not close to 1 or -1.

**Normality**

Inspecting the histograms, data is not normally distributed for both variables.

**Linearity**

After checking data meets assumptions, check the relationship between independent and dependent variables using linear regression.

**Homoscedasticity**

Plot the linear model results to check whether the observed tata meets our model assumptions.

Normal Q-Qplot doesn't a perfect one-to-one line with the theoretical residuals.

The red lines representing the mean of the residuals are not entirely horizontal.

---

# R Code

## Load and Randomly Sample Data

```{r setup libraries, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE)

# Libraries used
if(!require("readr"))
  install.packages("readr")
if(!require("dplyr"))
  install.packages("dplyr")
if(!require("ggplot2"))
  install.packages("ggplot2")
if(!require("knitr"))
  install.packages("knitr")
if(!require("kableExtra"))
  install.packages("kableExtra")

library(readr) # read_csv()
library(dplyr) # sample_n()
library(ggplot2) # data viszualizatoin, plot linear regression
library(knitr) # Display data in tables
library(kableExtra) # Format tables
```

```{r read csv file and summarise data}
# Load and Randomly Sample Data
# use readr::read_csv() to load data from csv file
data <- read_csv("amalgamated_game_survey_250_2022.csv") # read data from csv
summary(data) # Check data has been read in correctly
```

```{r randomly sample 200 of the 250 rows in the csv sample}
set.seed(1234) # set a seed to reproduce random values
# randomly sample 200 of the 250 rows
sample_data <- sample_n(data, 200) # returns tibble 200 x 11

```


## Calculate Linear Regression for Data

### 1. Independence of observation

Correlation / R Value

```{r}
# check the correlation between the chosen variables
cor(sample_data$avg_monthly_hrs_gaming, sample_data$avg_monthly_expenditure_dlc)
# perform correlation test on chosen variables
cor.test(sample_data$avg_monthly_hrs_gaming, sample_data$avg_monthly_expenditure_dlc)
```

### 2. Normality

**Histograms**

Check data visually with histograms.

```{r}
hist(sample_data$avg_monthly_hrs_gaming, 
     main="Average Monthly Hours Gaming Frequency",
     xlab="Average Monthly Hours Gaming")
```

Average Monthly Hours Gaming histogram skewed to the left slightly.

```{r}
hist(sample_data$avg_monthly_expenditure_dlc, 
     main="Average Monthly Expenditure DLC Fequency",
     xlab = "Average Monthly Expenditure DLC")
```

Roughly bell-shaped.

**Shapiro-Wilk's Method (Significance test)**

**null hypothesis:** the data are sampled from a Gaussian distribution.

```{r shapiro test}
significance <- 0.05
# If the P value is greater than 0.05 accept null hypothesis
# If the P value is less than or equal to 0.05 reject null hypothesis

st_hours <- shapiro.test(sample_data$avg_monthly_hrs_gaming)
# if shapiro test result is too low reject the null hypothesis
if(st_hours$p.value < significance) {
  print("reject") } else {
  print("accept")
}

st_bucks <- shapiro.test(sample_data$avg_monthly_expenditure_dlc)
# use ifelse() to perform similar check as above
print(ifelse(st_bucks$p.value < significance, "reject", "accept"))
```

Null hypothesis rejected for all variables before sampling

### 3. Linear Regression Analysis

```{r}
# simple linear regression
mod <- lm(avg_monthly_expenditure_dlc ~ avg_monthly_hrs_gaming, 
          data = sample_data) # calculate effect of independent on dependent variable
summary(mod) # summarise the results of the model
```

Not a Significant positive relationship between avg_monthly_hrs_gaming and avg_monthly_expenditure_dlc (p value > 0.05)

### 4. Check for homoscedasticity

```{r}
par(mfrow=c(2,2), main="test") # 2 rows and 2 columns
plot(mod, col.lab="blue", col.axis="blue") # plot the model
par(mfrow=c(1,1)) # Reset to 1 row and 1 column
```

Normal Q-Qplot doesn't a perfect one-to-one line with the theoretical residuals.

## Linear Regression Plot(s)

The plot is created using the linear model data to map the avg_monthly_hrs_gaming and avg_montly_expenditure_dlc variables as points in the plot.

The scale of the x and y axiis are set using the rounded down min value for the variable and the max value rounded up. With just rounded values they were showing with a decimal place and didn't look right.

```{r}
plot <- ggplot(data = mod, mapping = aes(x = avg_monthly_hrs_gaming, 
                                 y = avg_monthly_expenditure_dlc)) + 
  # plot dataset in a scatter plot, add colours for points
  geom_point(alpha = 0.66, # transparcency, lets stacked points show darker
             shape=21,# round
             fill="red", # inner colour
             color="black", # outline colour
             size=2.5) + # size (3 too big, 1 too small) +
  
  labs(title = "Relationship between games played + DLC expenditure",
       subtitle = "Average monthly values",
       caption = "Linear Regression Plot")

# Calculate x and y tick spacing and frequency
scale_x = scale_x_continuous(breaks = seq(
  floor(min(sample_data$avg_monthly_hrs_gaming)), # round down lowest value
  ceiling(max(sample_data$avg_monthly_hrs_gaming)), # round up highest value
  by = 2), # frequency
  name = "Average Monthly Hours Gaming") # x label
scale_y = scale_y_continuous(breaks = seq(
  floor(min(sample_data$avg_monthly_expenditure_dlc)), 
  ceiling(max(sample_data$avg_monthly_expenditure_dlc)),
  by = 5), 
  name = "Average Monthly Expenditure DLC")

# Get intercept and slope for regresion line
coeff <- coefficients(mod) # get coefficients returned from linear model
intercept <- coeff[1] # avg_monthly_hrs_gaming intercept
slope <- coeff[2] # slope of line

# Add x and y labels and geometry line to plot
plot + scale_x + scale_y +
  geom_abline(intercept = intercept, slope = slope, color="blue") # regression line
```

---

# References

**OpenIntro Statistics:**

M., D., D., C. and Ã‡etinkaya-Rundel, M., 2019. OpenIntro Statistics. OpenIntro, Incorporated.

OpenIntro Statistics. 2022. OpenIntro Statistics. [ONLINE] Available at: https://www.openintro.org/book/os/. [Accessed 18 January 2023].


**Linear Regression:**

Rebecca Bevans. 2023. Linear Regression in R | A Step-by-Step Guide & Examples. [ONLINE] Available at: https://www.scribbr.com/statistics/linear-regression-in-r/. [Accessed 18 January 2023].

R Programming 101 (YouTube). 2023. Linear regression using R programming - YouTube. [ONLINE] Available at: https://www.youtube.com/watch?v=-mGXnm0fHtI. [Accessed 19 January 2023].

**GGPlot**

R Programming 101 (YouTube) 2023. ggplot for plots and graphs. An introduction to data visualization using R programming - YouTube. [ONLINE] Available at: https://www.youtube.com/watch?v=HPJn1CMvtmI. [Accessed 19 January 2023].

Data Carpentry contributors. 2023. Data visualization with ggplot2. [ONLINE] Available at: https://datacarpentry.org/R-ecology-lesson/04-visualization-ggplot2.html. [Accessed 19 January 2023].
